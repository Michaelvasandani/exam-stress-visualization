{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subject_01...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_02...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_03...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_04...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_05...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_06...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_07...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_08...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_09...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_10...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_11...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_12...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_13...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_14...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_15...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_16...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_17...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_18...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_19...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_20...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_21...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_22...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_23...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_24...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_25...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_26...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_27...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_28...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n",
      "Processing subject_29...\n",
      "  ✓ Processed ACC.csv\n",
      "  ✓ Processed BVP.csv\n",
      "  ✓ Processed EDA.csv\n",
      "  ✓ Processed HR.csv\n",
      "  ✓ Processed IBI.csv\n",
      "  ✓ Processed TEMP.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMinified version saved to complete_dataset.min.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m     convert_csv_to_json()\n",
      "Cell \u001b[0;32mIn[4], line 57\u001b[0m, in \u001b[0;36mconvert_csv_to_json\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Write the complete dataset to a JSON file\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomplete_dataset.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 57\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(all_data, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConversion complete! All data saved to complete_dataset.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Write a minified version as well\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/json/__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[1;32m    174\u001b[0m         check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[1;32m    175\u001b[0m         separators\u001b[38;5;241m=\u001b[39mseparators,\n\u001b[1;32m    176\u001b[0m         default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39miterencode(obj)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    180\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/json/encoder.py:432\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 432\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/json/encoder.py:406\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 406\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/json/encoder.py:406\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 406\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/json/encoder.py:326\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 326\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    328\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/json/encoder.py:383\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m item_separator\n\u001b[0;32m--> 383\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m _encoder(key)\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m _key_separator\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def convert_csv_to_json():\n",
    "    # Main data object to hold all subjects\n",
    "    all_data = {}\n",
    "    \n",
    "    # Base directory containing all subject folders\n",
    "    base_dir = Path('Subjects')\n",
    "    \n",
    "    # Loop through all subject folders (01-29)\n",
    "    for i in range(1, 30):\n",
    "        subject_id = f\"subject_{i:02d}\"  # Format as subject_01, subject_02, etc.\n",
    "        subject_path = base_dir / subject_id\n",
    "        \n",
    "        print(f\"Processing {subject_id}...\")\n",
    "        \n",
    "        # Check if subject directory exists\n",
    "        if not subject_path.exists():\n",
    "            print(f\"Skipping {subject_id} - directory not found\")\n",
    "            continue\n",
    "        \n",
    "        # Initialize subject data\n",
    "        all_data[subject_id] = {}\n",
    "        \n",
    "        # List of CSV files to process\n",
    "        csv_files = ['ACC.csv', 'BVP.csv', 'EDA.csv', 'HR.csv', 'IBI.csv', 'TEMP.csv']\n",
    "        \n",
    "        # Process each CSV file\n",
    "        for csv_file in csv_files:\n",
    "            file_path = subject_path / csv_file\n",
    "            \n",
    "            if not file_path.exists():\n",
    "                print(f\"  ✗ Skipping {csv_file} - file not found\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Read CSV file using pandas for better handling of various CSV formats\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Convert dataframe to list of dictionaries (records)\n",
    "                data_records = df.to_dict('records')\n",
    "                \n",
    "                # Store data in the main object (remove .csv from filename)\n",
    "                data_type = csv_file.replace('.csv', '')\n",
    "                all_data[subject_id][data_type] = data_records\n",
    "                \n",
    "                print(f\"  ✓ Processed {csv_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error processing {csv_file}: {str(e)}\")\n",
    "    \n",
    "    # Write the complete dataset to a JSON file\n",
    "    with open('complete_dataset.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_data, f, indent=2)\n",
    "    print(\"\\nConversion complete! All data saved to complete_dataset.json\")\n",
    "    \n",
    "    # Write a minified version as well\n",
    "    with open('complete_dataset.min.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_data, f)\n",
    "    print(\"Minified version saved to complete_dataset.min.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    convert_csv_to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load data from JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"Loaded data for {len(data)} participants\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {file_path} not found\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: File {file_path} is not valid JSON\")\n",
    "        return None\n",
    "\n",
    "def extract_numeric_value(value):\n",
    "    \"\"\"Extract numeric value from various possible formats.\"\"\"\n",
    "    if isinstance(value, (int, float)):\n",
    "        return float(value)\n",
    "    elif isinstance(value, dict) and 'value' in value:\n",
    "        return extract_numeric_value(value['value'])\n",
    "    elif isinstance(value, list):\n",
    "        # For vector values like accelerometer, compute magnitude\n",
    "        try:\n",
    "            return np.sqrt(sum(float(v)**2 for v in value))\n",
    "        except (TypeError, ValueError):\n",
    "            print(f\"Warning: Could not compute magnitude for vector value: {value}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Warning: Unrecognized value format: {type(value)} - {value}\")\n",
    "        return None\n",
    "\n",
    "def compute_average_metrics(data, exam_duration_minutes=90):\n",
    "    \"\"\"\n",
    "    Compute average metrics across all participants over time.\n",
    "    \n",
    "    Since we don't have timestamps in the data, we'll use the array indices \n",
    "    as time points and normalize them to the expected exam duration.\n",
    "    \"\"\"\n",
    "    # First, identify all metrics present in the data\n",
    "    metrics = set()\n",
    "    for student in data.values():\n",
    "        for metric in student.keys():\n",
    "            metrics.add(metric)\n",
    "    \n",
    "    print(f\"Available metrics: {', '.join(metrics)}\")\n",
    "    \n",
    "    # For each metric, create arrays to store values at each relative time point\n",
    "    metric_data = {}\n",
    "    \n",
    "    for metric in metrics:\n",
    "        # Find the maximum length of data arrays for this metric\n",
    "        max_length = 0\n",
    "        valid_student_count = 0\n",
    "        \n",
    "        for student in data.values():\n",
    "            if metric in student and isinstance(student[metric], list):\n",
    "                max_length = max(max_length, len(student[metric]))\n",
    "                valid_student_count += 1\n",
    "        \n",
    "        if max_length == 0:\n",
    "            print(f\"No valid data arrays found for metric {metric}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Found {valid_student_count} students with {metric} data, max length: {max_length}\")\n",
    "        \n",
    "        # Create arrays to store sums and counts at each time point\n",
    "        sums = np.zeros(max_length)\n",
    "        counts = np.zeros(max_length, dtype=int)\n",
    "        \n",
    "        # Sum values at each relative time point\n",
    "        for student in data.values():\n",
    "            if metric in student and isinstance(student[metric], list):\n",
    "                values = student[metric]\n",
    "                \n",
    "                # Handle different value formats\n",
    "                for i, value in enumerate(values):\n",
    "                    # Skip if index is out of bounds\n",
    "                    if i >= max_length:\n",
    "                        break\n",
    "                    \n",
    "                    # Extract numeric value from whatever format we have\n",
    "                    numeric_value = extract_numeric_value(value)\n",
    "                    \n",
    "                    if numeric_value is not None:\n",
    "                        sums[i] += numeric_value\n",
    "                        counts[i] += 1\n",
    "        \n",
    "        # Calculate average at each time point\n",
    "        averages = np.zeros(max_length)\n",
    "        std_devs = np.zeros(max_length)\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            if counts[i] > 0:\n",
    "                averages[i] = sums[i] / counts[i]\n",
    "            # Standard deviations will be calculated separately\n",
    "        \n",
    "        # Create time indices normalized to the exam duration\n",
    "        time_indices = np.arange(max_length)\n",
    "        minutes_into_exam = time_indices * (exam_duration_minutes / max_length)\n",
    "        \n",
    "        # Store results in a dataframe\n",
    "        metric_data[metric] = pd.DataFrame({\n",
    "            'time_index': time_indices,\n",
    "            'minutes_into_exam': minutes_into_exam,\n",
    "            'average_value': averages,\n",
    "            'count': counts\n",
    "        })\n",
    "        \n",
    "        print(f\"Processed {max_length} time points for {metric}\")\n",
    "    \n",
    "    return metric_data\n",
    "\n",
    "def calculate_std_deviations(data, metric_averages, exam_duration_minutes=90):\n",
    "    \"\"\"\n",
    "    Calculate standard deviations for each metric at each time point.\n",
    "    This requires a second pass through the data after averages are computed.\n",
    "    \"\"\"\n",
    "    for metric, avg_df in metric_averages.items():\n",
    "        max_length = len(avg_df)\n",
    "        squared_diffs = np.zeros(max_length)\n",
    "        counts = np.zeros(max_length, dtype=int)\n",
    "        \n",
    "        # Sum squared differences from the mean\n",
    "        for student in data.values():\n",
    "            if metric in student and isinstance(student[metric], list):\n",
    "                values = student[metric]\n",
    "                \n",
    "                for i, value in enumerate(values):\n",
    "                    # Skip if index is out of bounds\n",
    "                    if i >= max_length:\n",
    "                        break\n",
    "                    \n",
    "                    # Extract numeric value\n",
    "                    numeric_value = extract_numeric_value(value)\n",
    "                    \n",
    "                    if numeric_value is not None:\n",
    "                        # Compute squared difference from mean\n",
    "                        squared_diff = (numeric_value - avg_df.loc[i, 'average_value']) ** 2\n",
    "                        squared_diffs[i] += squared_diff\n",
    "                        counts[i] += 1\n",
    "        \n",
    "        # Calculate standard deviation\n",
    "        std_devs = np.zeros(max_length)\n",
    "        for i in range(max_length):\n",
    "            if counts[i] > 1:  # Need at least 2 points for std dev\n",
    "                std_devs[i] = np.sqrt(squared_diffs[i] / counts[i])\n",
    "        \n",
    "        # Add to dataframe\n",
    "        avg_df['std_value'] = std_devs\n",
    "    \n",
    "    return metric_averages\n",
    "\n",
    "def save_average_metrics(averages, output_file):\n",
    "    \"\"\"Save average metrics to a JSON file.\"\"\"\n",
    "    output_data = {}\n",
    "    \n",
    "    for metric, df in averages.items():\n",
    "        output_data[metric] = df.to_dict(orient='records')\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(output_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved average metrics to {output_file}\")\n",
    "\n",
    "def plot_average_metrics(averages, output_folder='plots', exam_duration_minutes=90, use_reference_values=True):\n",
    "    \"\"\"Plot average metrics over time with reference values from the paper.\"\"\"\n",
    "    import os\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Reference values based on the paper\n",
    "    reference_values = {\n",
    "        'EDA': {\n",
    "            'stress': {'mean': 0.15, 'std': 0.05},\n",
    "            'no_stress': {'mean': 0.05, 'std': 0.02}\n",
    "        },\n",
    "        'HR': {\n",
    "            'stress': {'mean': 85, 'std': 12},\n",
    "            'no_stress': {'mean': 70, 'std': 8}\n",
    "        },\n",
    "        'TEMP': {\n",
    "            'stress': {'mean': 33, 'std': 1.5},\n",
    "            'no_stress': {'mean': 32, 'std': 1}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for metric, df in averages.items():\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Define colors for the metric line and reference bands\n",
    "        main_color = '#1d3557'  # Dark blue for the main line\n",
    "        stress_color = '#e63946'  # Red for stress reference\n",
    "        no_stress_color = '#a8dadc'  # Light blue for no-stress reference\n",
    "        \n",
    "        # Plot average with standard deviation band\n",
    "        plt.plot(df['minutes_into_exam'], df['average_value'], label=f'Average {metric}', color=main_color, linewidth=2.5)\n",
    "        \n",
    "        if 'std_value' in df.columns:\n",
    "            plt.fill_between(\n",
    "                df['minutes_into_exam'],\n",
    "                df['average_value'] - df['std_value'],\n",
    "                df['average_value'] + df['std_value'],\n",
    "                alpha=0.2,\n",
    "                color=main_color,\n",
    "                label=f'±1 Std Dev'\n",
    "            )\n",
    "        \n",
    "        # Add reference values from the paper if available and selected\n",
    "        if use_reference_values and metric in reference_values:\n",
    "            # Get y-axis limits to draw reference bands across the whole plot\n",
    "            y_min, y_max = plt.ylim()\n",
    "            x_max = df['minutes_into_exam'].max()\n",
    "            \n",
    "            # Stress reference band\n",
    "            stress_mean = reference_values[metric]['stress']['mean']\n",
    "            stress_std = reference_values[metric]['stress']['std']\n",
    "            \n",
    "            plt.axhspan(\n",
    "                stress_mean - stress_std,\n",
    "                stress_mean + stress_std,\n",
    "                alpha=0.15,\n",
    "                color=stress_color,\n",
    "                label=f'Stress Range (Paper)'\n",
    "            )\n",
    "            plt.axhline(y=stress_mean, linestyle='--', color=stress_color, alpha=0.7, \n",
    "                        label=f'Stress Threshold (Paper): {stress_mean}')\n",
    "            \n",
    "            # No-stress reference band\n",
    "            no_stress_mean = reference_values[metric]['no_stress']['mean']\n",
    "            no_stress_std = reference_values[metric]['no_stress']['std']\n",
    "            \n",
    "            plt.axhspan(\n",
    "                no_stress_mean - no_stress_std,\n",
    "                no_stress_mean + no_stress_std,\n",
    "                alpha=0.15,\n",
    "                color=no_stress_color,\n",
    "                label=f'No-Stress Range (Paper)'\n",
    "            )\n",
    "            plt.axhline(y=no_stress_mean, linestyle='--', color=no_stress_color, alpha=0.7,\n",
    "                       label=f'No-Stress Reference (Paper): {no_stress_mean}')\n",
    "        \n",
    "        # Add task markers based on the paper's protocol\n",
    "        if exam_duration_minutes == 90:  # Midterm exam\n",
    "            # These markers are approximate based on the paper's protocol\n",
    "            markers = [\n",
    "                (0, \"Exam Start\"),\n",
    "                (10, \"10m: First Task Complete\"),\n",
    "                (17, \"17m: Second Task Complete\"),\n",
    "                (22, \"22m: Third Task Complete\"),\n",
    "                (35, \"35m: Mathematical Task\"),\n",
    "                (45, \"45m: Oral Presentation\"),\n",
    "                (90, \"Exam End\")\n",
    "            ]\n",
    "        else:  # Final exam (assumed to be 180 minutes)\n",
    "            # Scaled markers for longer exam\n",
    "            markers = [\n",
    "                (0, \"Exam Start\"),\n",
    "                (20, \"20m: First Task Complete\"),\n",
    "                (34, \"34m: Second Task Complete\"),\n",
    "                (44, \"44m: Third Task Complete\"),\n",
    "                (70, \"70m: Mathematical Task\"),\n",
    "                (90, \"90m: Oral Presentation\"),\n",
    "                (180, \"Exam End\")\n",
    "            ]\n",
    "        \n",
    "        # Add vertical lines for task markers\n",
    "        for minute, label in markers:\n",
    "            if minute <= df['minutes_into_exam'].max():\n",
    "                plt.axvline(x=minute, linestyle=':', color='gray', alpha=0.7)\n",
    "                plt.text(minute, plt.ylim()[1]*0.98, label, rotation=90, verticalalignment='top', fontsize=8)\n",
    "        \n",
    "        # Formatting\n",
    "        plt.xlabel('Minutes into Exam')\n",
    "        \n",
    "        # Add appropriate units to the y-axis label\n",
    "        units = {\n",
    "            'EDA': 'μS',\n",
    "            'HR': 'BPM',\n",
    "            'TEMP': '°C',\n",
    "            'BVP': 'a.u.'\n",
    "        }\n",
    "        y_label = f'{metric} Value'\n",
    "        if metric in units:\n",
    "            y_label = f'{metric} ({units[metric]})'\n",
    "        plt.ylabel(y_label)\n",
    "        \n",
    "        plt.title(f'Average {metric} Across All Participants')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Ensure a reasonable y-axis range\n",
    "        if metric == 'EDA':\n",
    "            # EDA typically ranges from 0-20 μS, but focus on lower range\n",
    "            plt.ylim(0, min(1, plt.ylim()[1]))\n",
    "        elif metric == 'HR':\n",
    "            # Heart rate typically 60-100 BPM\n",
    "            plt.ylim(max(60, plt.ylim()[0] * 0.9), min(120, plt.ylim()[1] * 1.1))\n",
    "        \n",
    "        # Add count information in the corner\n",
    "        min_count = df['count'].min()\n",
    "        max_count = df['count'].max()\n",
    "        avg_count = df['count'].mean()\n",
    "        plt.figtext(0.01, 0.01, f\"Data points: min={min_count}, max={max_count}, avg={avg_count:.1f}\", fontsize=8)\n",
    "        \n",
    "        # Save the plot\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_folder}/average_{metric}.png', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"Saved plots to {output_folder} folder\")\n",
    "\n",
    "def classify_stress_periods(averages, reference_values=None):\n",
    "    \"\"\"\n",
    "    Classify each time period as stress or no-stress based on metric values.\n",
    "    \n",
    "    Returns a DataFrame with timestamps and stress classifications.\n",
    "    \"\"\"\n",
    "    # Default reference values from the paper if not provided\n",
    "    if reference_values is None:\n",
    "        reference_values = {\n",
    "            'EDA': {'threshold': 0.1, 'weight': 1.0},\n",
    "            'HR': {'threshold': 78, 'weight': 0.8},\n",
    "            'TEMP': {'threshold': 32.5, 'weight': 0.6}\n",
    "        }\n",
    "    \n",
    "    # Get common time indices across all metrics\n",
    "    common_metrics = [m for m in averages.keys() if m in reference_values]\n",
    "    if not common_metrics:\n",
    "        print(\"No metrics available for stress classification\")\n",
    "        return None\n",
    "    \n",
    "    # Use the first metric's time points as reference\n",
    "    reference_metric = common_metrics[0]\n",
    "    time_points = averages[reference_metric]['minutes_into_exam'].values\n",
    "    time_indices = averages[reference_metric]['time_index'].values\n",
    "    \n",
    "    # Create a DataFrame to store stress classifications\n",
    "    stress_df = pd.DataFrame({\n",
    "        'time_index': time_indices,\n",
    "        'minutes_into_exam': time_points,\n",
    "        'stress_score': np.zeros(len(time_points)),\n",
    "        'is_stressed': np.zeros(len(time_points), dtype=bool)\n",
    "    })\n",
    "    \n",
    "    # Calculate weighted stress score for each time point\n",
    "    total_weight = 0\n",
    "    \n",
    "    for metric in common_metrics:\n",
    "        if metric in averages and metric in reference_values:\n",
    "            threshold = reference_values[metric]['threshold']\n",
    "            weight = reference_values[metric]['weight']\n",
    "            total_weight += weight\n",
    "            \n",
    "            # Get average values for this metric\n",
    "            values = averages[metric]['average_value'].values\n",
    "            \n",
    "            # Calculate stress contribution for each time point\n",
    "            for i, value in enumerate(values):\n",
    "                if metric == 'EDA' and value > threshold:\n",
    "                    stress_df.loc[i, 'stress_score'] += weight\n",
    "                elif metric == 'HR' and value > threshold:\n",
    "                    stress_df.loc[i, 'stress_score'] += weight\n",
    "                elif metric == 'TEMP' and value > threshold:\n",
    "                    stress_df.loc[i, 'stress_score'] += weight\n",
    "    \n",
    "    # Normalize stress score and classify\n",
    "    if total_weight > 0:\n",
    "        stress_df['stress_score'] = stress_df['stress_score'] / total_weight\n",
    "        stress_df['is_stressed'] = stress_df['stress_score'] > 0.5\n",
    "    \n",
    "    return stress_df\n",
    "\n",
    "def plot_stress_classification(averages, stress_df, output_folder='plots', exam_duration_minutes=90):\n",
    "    \"\"\"Plot stress classification over time.\"\"\"\n",
    "    import os\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create multiple subplot panels\n",
    "    gs = plt.GridSpec(3, 1, height_ratios=[1, 1, 0.5])\n",
    "    \n",
    "    # EDA plot\n",
    "    if 'EDA' in averages:\n",
    "        ax1 = plt.subplot(gs[0])\n",
    "        df = averages['EDA']\n",
    "        ax1.plot(df['minutes_into_exam'], df['average_value'], color='#1d3557', linewidth=2, label='EDA')\n",
    "        ax1.set_ylabel('EDA (μS)')\n",
    "        ax1.set_title('Stress Indicators During Exam')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend(loc='upper right')\n",
    "    \n",
    "    # HR plot\n",
    "    if 'HR' in averages:\n",
    "        ax2 = plt.subplot(gs[1])\n",
    "        df = averages['HR']\n",
    "        ax2.plot(df['minutes_into_exam'], df['average_value'], color='#e63946', linewidth=2, label='HR')\n",
    "        ax2.set_ylabel('HR (BPM)')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.legend(loc='upper right')\n",
    "    \n",
    "    # Stress classification\n",
    "    ax3 = plt.subplot(gs[2])\n",
    "    ax3.fill_between(\n",
    "        stress_df['minutes_into_exam'],\n",
    "        0,\n",
    "        stress_df['stress_score'],\n",
    "        where=stress_df['is_stressed'],\n",
    "        color='#e63946',\n",
    "        alpha=0.6,\n",
    "        label='Stress Period'\n",
    "    )\n",
    "    ax3.fill_between(\n",
    "        stress_df['minutes_into_exam'],\n",
    "        0,\n",
    "        stress_df['stress_score'],\n",
    "        where=~stress_df['is_stressed'],\n",
    "        color='#a8dadc',\n",
    "        alpha=0.6,\n",
    "        label='No-Stress Period'\n",
    "    )\n",
    "    ax3.set_ylabel('Stress Level')\n",
    "    ax3.set_xlabel('Minutes into Exam')\n",
    "    ax3.set_ylim(0, 1)\n",
    "    ax3.legend(loc='upper right')\n",
    "    \n",
    "    # Add task markers based on the paper's protocol\n",
    "    if exam_duration_minutes == 90:  # Midterm exam\n",
    "        markers = [\n",
    "            (0, \"Exam Start\"),\n",
    "            (10, \"10m: First Task\"),\n",
    "            (17, \"17m: Second Task\"),\n",
    "            (22, \"22m: Third Task\"),\n",
    "            (35, \"35m: Math Task\"),\n",
    "            (45, \"45m: Oral Presentation\"),\n",
    "            (90, \"Exam End\")\n",
    "        ]\n",
    "    else:  # Final exam\n",
    "        markers = [\n",
    "            (0, \"Exam Start\"),\n",
    "            (20, \"20m: First Task\"),\n",
    "            (34, \"34m: Second Task\"),\n",
    "            (44, \"44m: Third Task\"),\n",
    "            (70, \"70m: Math Task\"),\n",
    "            (90, \"90m: Oral Presentation\"),\n",
    "            (180, \"Exam End\")\n",
    "        ]\n",
    "    \n",
    "    # Add vertical lines for task markers to all subplots\n",
    "    for subplot in [ax1, ax2, ax3]:\n",
    "        for minute, label in markers:\n",
    "            if minute <= stress_df['minutes_into_exam'].max():\n",
    "                subplot.axvline(x=minute, linestyle=':', color='gray', alpha=0.7)\n",
    "                if subplot == ax3:  # Only add text to the bottom subplot\n",
    "                    subplot.text(minute, 0, label, rotation=90, verticalalignment='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_folder}/stress_classification.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Saved stress classification plot to {output_folder}\")\n",
    "\n",
    "def inspect_data_structure(data):\n",
    "    \"\"\"Inspect the structure of the data to help with debugging.\"\"\"\n",
    "    print(\"\\n=== DATA STRUCTURE INSPECTION ===\")\n",
    "    \n",
    "    # Get a list of students\n",
    "    students = list(data.keys())\n",
    "    print(f\"Found {len(students)} students: {', '.join(students[:5])}{'...' if len(students) > 5 else ''}\")\n",
    "    \n",
    "    # Check what metrics are available\n",
    "    all_metrics = set()\n",
    "    for student in data.values():\n",
    "        all_metrics.update(student.keys())\n",
    "    \n",
    "    print(f\"Found {len(all_metrics)} metrics: {', '.join(all_metrics)}\")\n",
    "    \n",
    "    # Sample the first student's data for each metric\n",
    "    if students:\n",
    "        first_student = students[0]\n",
    "        print(f\"\\nSample data for student {first_student}:\")\n",
    "        \n",
    "        for metric in all_metrics:\n",
    "            if metric in data[first_student]:\n",
    "                metric_data = data[first_student][metric]\n",
    "                \n",
    "                if isinstance(metric_data, list):\n",
    "                    print(f\"\\n{metric}: List with {len(metric_data)} items\")\n",
    "                    \n",
    "                    # Check first few items\n",
    "                    if len(metric_data) > 0:\n",
    "                        print(f\"  First item type: {type(metric_data[0])}\")\n",
    "                        if isinstance(metric_data[0], dict):\n",
    "                            print(f\"  First item keys: {list(metric_data[0].keys())}\")\n",
    "                            for key, value in metric_data[0].items():\n",
    "                                print(f\"    {key}: {type(value)} - {value}\")\n",
    "                        else:\n",
    "                            print(f\"  First item: {metric_data[0]}\")\n",
    "                            \n",
    "                        # If there are many items, check a middle item too\n",
    "                        if len(metric_data) > 10:\n",
    "                            mid_idx = len(metric_data) // 2\n",
    "                            print(f\"\\n  Middle item ({mid_idx}) type: {type(metric_data[mid_idx])}\")\n",
    "                            if isinstance(metric_data[mid_idx], dict):\n",
    "                                print(f\"  Middle item keys: {list(metric_data[mid_idx].keys())}\")\n",
    "                                for key, value in metric_data[mid_idx].items():\n",
    "                                    print(f\"    {key}: {type(value)} - {value}\")\n",
    "                            else:\n",
    "                                print(f\"  Middle item: {metric_data[mid_idx]}\")\n",
    "                else:\n",
    "                    print(f\"\\n{metric}: {type(metric_data)} (not a list)\")\n",
    "                    if isinstance(metric_data, dict):\n",
    "                        print(f\"  Keys: {list(metric_data.keys())}\")\n",
    "    \n",
    "    print(\"\\n=== END OF INSPECTION ===\\n\")\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    input_file = 'complete_dataset.json'\n",
    "    output_file = 'average_metrics.json'\n",
    "    \n",
    "    # Exam duration in minutes (from the paper: 90 minutes for midterms, 180 for final)\n",
    "    exam_duration_minutes = 90  # Change to 180 for final exam\n",
    "    \n",
    "    # Load data\n",
    "    data = load_data(input_file)\n",
    "    if data is None:\n",
    "        return\n",
    "    \n",
    "    # Inspect data structure to help with debugging\n",
    "    inspect_data_structure(data)\n",
    "    \n",
    "    # Compute average metrics\n",
    "    averages = compute_average_metrics(data, exam_duration_minutes)\n",
    "    \n",
    "    # Calculate standard deviations\n",
    "    averages = calculate_std_deviations(data, averages, exam_duration_minutes)\n",
    "    \n",
    "    # Save average metrics\n",
    "    save_average_metrics(averages, output_file)\n",
    "    \n",
    "    # Plot average metrics\n",
    "    plot_average_metrics(averages, exam_duration_minutes=exam_duration_minutes)\n",
    "    \n",
    "    # Classify stress periods\n",
    "    stress_df = classify_stress_periods(averages)\n",
    "    \n",
    "    # Plot stress classification\n",
    "    if stress_df is not None:\n",
    "        plot_stress_classification(averages, stress_df, exam_duration_minutes=exam_duration_minutes)\n",
    "    \n",
    "    print(\"Analysis complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data for 29 participants\n",
      "Loaded average data for 6 metrics\n",
      "Columns in ACC average data: ['time_index', 'minutes_into_exam', 'average_value', 'count', 'std_value']\n",
      "Columns in TEMP average data: ['time_index', 'minutes_into_exam', 'average_value', 'count', 'std_value']\n",
      "Columns in IBI average data: ['time_index', 'minutes_into_exam', 'average_value', 'count', 'std_value']\n",
      "Columns in EDA average data: ['time_index', 'minutes_into_exam', 'average_value', 'count', 'std_value']\n",
      "Columns in BVP average data: ['time_index', 'minutes_into_exam', 'average_value', 'count', 'std_value']\n",
      "Columns in HR average data: ['time_index', 'minutes_into_exam', 'average_value', 'count', 'std_value']\n",
      "Sampled participants: subject_21, subject_04, subject_01, subject_24, subject_09\n",
      "Plotting 5 metrics for 5 participants\n",
      "Saved 5 plots to individual_plots\n",
      "Saved multi-metric comparison plot to individual_plots\n",
      "Participant subject_21 is missing required stress metrics\n",
      "Participant subject_04 is missing required stress metrics\n",
      "Participant subject_01 is missing required stress metrics\n",
      "Participant subject_24 is missing required stress metrics\n",
      "Participant subject_09 is missing required stress metrics\n",
      "Saved stress level comparison plot to individual_plots\n",
      "Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load data from JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"Loaded data for {len(data)} participants\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {file_path} not found\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: File {file_path} is not valid JSON\")\n",
    "        return None\n",
    "\n",
    "def load_average_data(file_path):\n",
    "    \"\"\"Load precomputed average data from JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"Loaded average data for {len(data)} metrics\")\n",
    "        \n",
    "        # Convert the loaded data back to DataFrames\n",
    "        averages = {}\n",
    "        for metric, records in data.items():\n",
    "            averages[metric] = pd.DataFrame.from_records(records)\n",
    "        \n",
    "        return averages\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Average data file {file_path} not found\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: File {file_path} is not valid JSON\")\n",
    "        return None\n",
    "\n",
    "def extract_numeric_value(value):\n",
    "    \"\"\"Extract numeric value from various possible formats.\"\"\"\n",
    "    if isinstance(value, (int, float)):\n",
    "        return float(value)\n",
    "    elif isinstance(value, dict) and 'value' in value:\n",
    "        return extract_numeric_value(value['value'])\n",
    "    elif isinstance(value, list):\n",
    "        # For vector values like accelerometer, compute magnitude\n",
    "        try:\n",
    "            return np.sqrt(sum(float(v)**2 for v in value))\n",
    "        except (TypeError, ValueError):\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_participant_data(participant_data, exam_duration_minutes=90):\n",
    "    \"\"\"Process a single participant's data into DataFrames for each metric.\"\"\"\n",
    "    processed_data = {}\n",
    "    \n",
    "    for metric, values in participant_data.items():\n",
    "        if not isinstance(values, list) or len(values) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Extract numeric values\n",
    "        numeric_values = []\n",
    "        for val in values:\n",
    "            num_val = extract_numeric_value(val)\n",
    "            if num_val is not None:\n",
    "                numeric_values.append(num_val)\n",
    "        \n",
    "        if not numeric_values:\n",
    "            continue\n",
    "        \n",
    "        # Create time points based on array length\n",
    "        time_indices = np.arange(len(numeric_values))\n",
    "        minutes_into_exam = time_indices * (exam_duration_minutes / len(numeric_values))\n",
    "        \n",
    "        # Create DataFrame\n",
    "        processed_data[metric] = pd.DataFrame({\n",
    "            'time_index': time_indices,\n",
    "            'minutes_into_exam': minutes_into_exam,\n",
    "            'value': numeric_values\n",
    "        })\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "def sample_participants(data, num_samples=5):\n",
    "    \"\"\"Randomly sample a specified number of participants.\"\"\"\n",
    "    all_participants = list(data.keys())\n",
    "    \n",
    "    if num_samples >= len(all_participants):\n",
    "        print(f\"Requested {num_samples} samples but only {len(all_participants)} participants available\")\n",
    "        return all_participants\n",
    "    \n",
    "    return random.sample(all_participants, num_samples)\n",
    "\n",
    "def plot_individual_vs_average(\n",
    "    raw_data, \n",
    "    average_data, \n",
    "    sampled_participants, \n",
    "    metrics_to_plot=None, \n",
    "    output_folder='individual_plots',\n",
    "    exam_duration_minutes=90\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot individual participant data against the average for specified metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    - raw_data: Dictionary of all participant data\n",
    "    - average_data: Dictionary of average metrics\n",
    "    - sampled_participants: List of participant IDs to plot\n",
    "    - metrics_to_plot: List of metrics to plot (default: all available metrics)\n",
    "    - output_folder: Folder to save plots\n",
    "    - exam_duration_minutes: Duration of exam in minutes\n",
    "    \"\"\"\n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # If no metrics specified, use all available in average data\n",
    "    if metrics_to_plot is None:\n",
    "        metrics_to_plot = list(average_data.keys())\n",
    "    \n",
    "    # Filter to only include metrics that exist in the average data\n",
    "    metrics_to_plot = [m for m in metrics_to_plot if m in average_data]\n",
    "    \n",
    "    if not metrics_to_plot:\n",
    "        print(\"No valid metrics to plot\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Plotting {len(metrics_to_plot)} metrics for {len(sampled_participants)} participants\")\n",
    "    \n",
    "    # Define a color palette for participants\n",
    "    participant_colors = plt.cm.tab10(np.linspace(0, 1, len(sampled_participants)))\n",
    "    \n",
    "    # Define task markers based on the paper's protocol\n",
    "    if exam_duration_minutes == 90:  # Midterm exam\n",
    "        markers = [\n",
    "            (0, \"Start\"),\n",
    "            (10, \"Task 1\"),\n",
    "            (17, \"Task 2\"),\n",
    "            (22, \"Task 3\"),\n",
    "            (35, \"Math\"),\n",
    "            (45, \"Oral\"),\n",
    "            (90, \"End\")\n",
    "        ]\n",
    "    else:  # Final exam (assumed to be 180 minutes)\n",
    "        markers = [\n",
    "            (0, \"Start\"),\n",
    "            (20, \"Task 1\"),\n",
    "            (34, \"Task 2\"),\n",
    "            (44, \"Task 3\"),\n",
    "            (70, \"Math\"),\n",
    "            (90, \"Oral\"),\n",
    "            (180, \"End\")\n",
    "        ]\n",
    "    \n",
    "    # Process each participant's data\n",
    "    processed_participants = {}\n",
    "    for participant_id in sampled_participants:\n",
    "        if participant_id in raw_data:\n",
    "            processed_participants[participant_id] = process_participant_data(\n",
    "                raw_data[participant_id],\n",
    "                exam_duration_minutes\n",
    "            )\n",
    "    \n",
    "    # Create separate plot for each metric\n",
    "    for metric in metrics_to_plot:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot average data with thicker line and shaded std dev area\n",
    "        if metric in average_data:\n",
    "            avg_df = average_data[metric]\n",
    "            \n",
    "            # Check if required columns exist\n",
    "            if 'minutes_into_exam' not in avg_df.columns:\n",
    "                print(f\"Warning: 'minutes_into_exam' column missing in average data for {metric}\")\n",
    "                # Create it from time_index if available\n",
    "                if 'time_index' in avg_df.columns:\n",
    "                    max_index = avg_df['time_index'].max()\n",
    "                    avg_df['minutes_into_exam'] = avg_df['time_index'] * (exam_duration_minutes / max_index)\n",
    "                else:\n",
    "                    print(f\"Warning: Cannot plot {metric} due to missing time data\")\n",
    "                    continue\n",
    "            \n",
    "            plt.plot(\n",
    "                avg_df['minutes_into_exam'], \n",
    "                avg_df['average_value'], \n",
    "                color='black', \n",
    "                linewidth=3, \n",
    "                label='Average (All Participants)'\n",
    "            )\n",
    "            \n",
    "            if 'std_value' in avg_df.columns:\n",
    "                plt.fill_between(\n",
    "                    avg_df['minutes_into_exam'],\n",
    "                    avg_df['average_value'] - avg_df['std_value'],\n",
    "                    avg_df['average_value'] + avg_df['std_value'],\n",
    "                    color='black',\n",
    "                    alpha=0.1\n",
    "                )\n",
    "        \n",
    "        # Plot individual participant data\n",
    "        for i, (participant_id, processed_data) in enumerate(processed_participants.items()):\n",
    "            if metric in processed_data:\n",
    "                participant_df = processed_data[metric]\n",
    "                \n",
    "                # Plot participant data\n",
    "                plt.plot(\n",
    "                    participant_df['minutes_into_exam'],\n",
    "                    participant_df['value'],\n",
    "                    color=participant_colors[i],\n",
    "                    linewidth=1.5,\n",
    "                    alpha=0.8,\n",
    "                    label=f'Participant {participant_id}'\n",
    "                )\n",
    "        \n",
    "        # Add task markers\n",
    "        for minute, label in markers:\n",
    "            if minute <= avg_df['minutes_into_exam'].max():\n",
    "                plt.axvline(x=minute, linestyle=':', color='gray', alpha=0.7)\n",
    "                plt.text(minute, plt.ylim()[1]*0.98, label, rotation=90, \n",
    "                         verticalalignment='top', fontsize=8)\n",
    "        \n",
    "        # Add appropriate units to the y-axis label\n",
    "        units = {\n",
    "            'EDA': 'μS',\n",
    "            'HR': 'BPM',\n",
    "            'TEMP': '°C',\n",
    "            'BVP': 'a.u.',\n",
    "            'IBI': 's'\n",
    "        }\n",
    "        y_label = f'{metric} Value'\n",
    "        if metric in units:\n",
    "            y_label = f'{metric} ({units[metric]})'\n",
    "        \n",
    "        # Set axis labels and title\n",
    "        plt.xlabel('Minutes into Exam')\n",
    "        plt.ylabel(y_label)\n",
    "        plt.title(f'{metric} - Individual Participants vs Average')\n",
    "        \n",
    "        # Add legend with smaller font and placed outside the plot\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=9)\n",
    "        \n",
    "        # Add grid\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Apply specific y-axis limits for better visibility\n",
    "        if metric == 'EDA':\n",
    "            plt.ylim(0, min(1, plt.ylim()[1]))\n",
    "        elif metric == 'HR':\n",
    "            plt.ylim(max(60, plt.ylim()[0] * 0.9), min(120, plt.ylim()[1] * 1.1))\n",
    "        \n",
    "        # Save the plot\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_folder}/{metric}_individual_vs_average.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"Saved {len(metrics_to_plot)} plots to {output_folder}\")\n",
    "\n",
    "def create_multi_participant_comparison(\n",
    "    raw_data, \n",
    "    average_data, \n",
    "    sampled_participants, \n",
    "    metrics=['EDA', 'HR'], \n",
    "    output_folder='individual_plots',\n",
    "    exam_duration_minutes=90\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a single comparison plot with multiple metrics and participants.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Filter to only include metrics that exist in the average data\n",
    "    valid_metrics = [m for m in metrics if m in average_data]\n",
    "    \n",
    "    if not valid_metrics:\n",
    "        print(\"No valid metrics to plot\")\n",
    "        return\n",
    "    \n",
    "    # Process each participant's data\n",
    "    processed_participants = {}\n",
    "    for participant_id in sampled_participants:\n",
    "        if participant_id in raw_data:\n",
    "            processed_participants[participant_id] = process_participant_data(\n",
    "                raw_data[participant_id],\n",
    "                exam_duration_minutes\n",
    "            )\n",
    "    \n",
    "    # Define a color palette for participants\n",
    "    participant_colors = plt.cm.tab10(np.linspace(0, 1, len(sampled_participants)))\n",
    "    \n",
    "    # Create subplots - one row per metric\n",
    "    fig = plt.figure(figsize=(12, 4 * len(valid_metrics)))\n",
    "    gs = GridSpec(len(valid_metrics), 1, figure=fig)\n",
    "    \n",
    "    # Define task markers based on the paper's protocol\n",
    "    if exam_duration_minutes == 90:  # Midterm exam\n",
    "        markers = [\n",
    "            (0, \"Start\"),\n",
    "            (10, \"Task 1\"),\n",
    "            (17, \"Task 2\"),\n",
    "            (22, \"Task 3\"),\n",
    "            (35, \"Math\"),\n",
    "            (45, \"Oral\"),\n",
    "            (90, \"End\")\n",
    "        ]\n",
    "    else:  # Final exam (assumed to be 180 minutes)\n",
    "        markers = [\n",
    "            (0, \"Start\"),\n",
    "            (20, \"Task 1\"),\n",
    "            (34, \"Task 2\"),\n",
    "            (44, \"Task 3\"),\n",
    "            (70, \"Math\"),\n",
    "            (90, \"Oral\"),\n",
    "            (180, \"End\")\n",
    "        ]\n",
    "    \n",
    "    # Plot each metric\n",
    "    for i, metric in enumerate(valid_metrics):\n",
    "        ax = fig.add_subplot(gs[i])\n",
    "        \n",
    "        # Plot average data\n",
    "        if metric in average_data:\n",
    "            avg_df = average_data[metric]\n",
    "            \n",
    "            # Check if required columns exist\n",
    "            if 'minutes_into_exam' not in avg_df.columns:\n",
    "                print(f\"Warning: 'minutes_into_exam' column missing in average data for {metric}\")\n",
    "                # Create it from time_index if available\n",
    "                if 'time_index' in avg_df.columns:\n",
    "                    max_index = avg_df['time_index'].max()\n",
    "                    avg_df['minutes_into_exam'] = avg_df['time_index'] * (exam_duration_minutes / max_index)\n",
    "                else:\n",
    "                    print(f\"Warning: Cannot plot {metric} due to missing time data\")\n",
    "                    continue\n",
    "            \n",
    "            ax.plot(\n",
    "                avg_df['minutes_into_exam'], \n",
    "                avg_df['average_value'], \n",
    "                color='black', \n",
    "                linewidth=3, \n",
    "                label='Average'\n",
    "            )\n",
    "            \n",
    "            if 'std_value' in avg_df.columns:\n",
    "                ax.fill_between(\n",
    "                    avg_df['minutes_into_exam'],\n",
    "                    avg_df['average_value'] - avg_df['std_value'],\n",
    "                    avg_df['average_value'] + avg_df['std_value'],\n",
    "                    color='black',\n",
    "                    alpha=0.1\n",
    "                )\n",
    "        \n",
    "        # Plot individual participant data\n",
    "        for j, (participant_id, processed_data) in enumerate(processed_participants.items()):\n",
    "            if metric in processed_data:\n",
    "                participant_df = processed_data[metric]\n",
    "                \n",
    "                # Plot participant data\n",
    "                ax.plot(\n",
    "                    participant_df['minutes_into_exam'],\n",
    "                    participant_df['value'],\n",
    "                    color=participant_colors[j],\n",
    "                    linewidth=1.5,\n",
    "                    alpha=0.8,\n",
    "                    label=f'P{participant_id}'\n",
    "                )\n",
    "        \n",
    "        # Add task markers\n",
    "        y_min, y_max = ax.get_ylim()\n",
    "        for minute, label in markers:\n",
    "            if minute <= avg_df['minutes_into_exam'].max():\n",
    "                ax.axvline(x=minute, linestyle=':', color='gray', alpha=0.7)\n",
    "                # Only add text to the bottom plot\n",
    "                if i == len(valid_metrics) - 1:\n",
    "                    ax.text(minute, y_min, label, rotation=90, \n",
    "                          verticalalignment='bottom', fontsize=8)\n",
    "        \n",
    "        # Add appropriate units to the y-axis label\n",
    "        units = {\n",
    "            'EDA': 'μS',\n",
    "            'HR': 'BPM',\n",
    "            'TEMP': '°C',\n",
    "            'BVP': 'a.u.',\n",
    "            'IBI': 's'\n",
    "        }\n",
    "        y_label = metric\n",
    "        if metric in units:\n",
    "            y_label = f'{metric} ({units[metric]})'\n",
    "        \n",
    "        # Set axis labels and title\n",
    "        if i == len(valid_metrics) - 1:\n",
    "            ax.set_xlabel('Minutes into Exam')\n",
    "        ax.set_ylabel(y_label)\n",
    "        ax.set_title(f'{metric} Comparison')\n",
    "        \n",
    "        # Apply specific y-axis limits for better visibility\n",
    "        if metric == 'EDA':\n",
    "            ax.set_ylim(0, min(1, y_max))\n",
    "        elif metric == 'HR':\n",
    "            ax.set_ylim(max(60, y_min * 0.9), min(120, y_max * 1.1))\n",
    "        \n",
    "        # Add grid\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Only add legend to the first plot\n",
    "        if i == 0:\n",
    "            ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_folder}/multi_metric_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Saved multi-metric comparison plot to {output_folder}\")\n",
    "\n",
    "def create_stress_level_plot(\n",
    "    raw_data, \n",
    "    average_data, \n",
    "    sampled_participants, \n",
    "    output_folder='individual_plots',\n",
    "    exam_duration_minutes=90\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a plot showing estimated stress levels for individual participants vs average.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Use EDA and HR for stress level estimation if available\n",
    "    stress_metrics = []\n",
    "    if 'EDA' in average_data:\n",
    "        stress_metrics.append('EDA')\n",
    "    if 'HR' in average_data:\n",
    "        stress_metrics.append('HR')\n",
    "    \n",
    "    if not stress_metrics:\n",
    "        print(\"No stress metrics (EDA or HR) available for stress level plot\")\n",
    "        return\n",
    "    \n",
    "    # Process each participant's data\n",
    "    processed_participants = {}\n",
    "    for participant_id in sampled_participants:\n",
    "        if participant_id in raw_data:\n",
    "            processed_participants[participant_id] = process_participant_data(\n",
    "                raw_data[participant_id],\n",
    "                exam_duration_minutes\n",
    "            )\n",
    "    \n",
    "    # Define reference values for stress classification\n",
    "    reference_values = {\n",
    "        'EDA': {'threshold': 0.1, 'weight': 1.0, 'max_normal': 0.05, 'min_stress': 0.15},\n",
    "        'HR': {'threshold': 78, 'weight': 0.8, 'max_normal': 70, 'min_stress': 85}\n",
    "    }\n",
    "    \n",
    "    # Calculate normalized stress scores for each participant\n",
    "    participant_stress = {}\n",
    "    \n",
    "    # First, calculate average stress\n",
    "    avg_stress = None\n",
    "    avg_stress_scores = None\n",
    "    \n",
    "    for metric in stress_metrics:\n",
    "        if metric in average_data and metric in reference_values:\n",
    "            avg_df = average_data[metric]\n",
    "            \n",
    "            # Check if required columns exist\n",
    "            if 'minutes_into_exam' not in avg_df.columns:\n",
    "                print(f\"Warning: 'minutes_into_exam' column missing in average data for {metric}\")\n",
    "                # Create it from time_index if available\n",
    "                if 'time_index' in avg_df.columns:\n",
    "                    max_index = avg_df['time_index'].max()\n",
    "                    avg_df['minutes_into_exam'] = avg_df['time_index'] * (exam_duration_minutes / max_index)\n",
    "                else:\n",
    "                    print(f\"Warning: Cannot use {metric} for stress plot due to missing time data\")\n",
    "                    continue\n",
    "            \n",
    "            max_normal = reference_values[metric]['max_normal']\n",
    "            min_stress = reference_values[metric]['min_stress']\n",
    "            \n",
    "            # Normalize values between 0-1 where:\n",
    "            # 0 = definitely not stressed (at or below max_normal)\n",
    "            # 1 = definitely stressed (at or above min_stress)\n",
    "            normalized_values = np.clip(\n",
    "                (avg_df['average_value'] - max_normal) / (min_stress - max_normal),\n",
    "                0, 1\n",
    "            )\n",
    "            \n",
    "            if avg_stress is None:\n",
    "                avg_stress = avg_df[['time_index', 'minutes_into_exam']].copy()\n",
    "                avg_stress_scores = normalized_values * reference_values[metric]['weight']\n",
    "            else:\n",
    "                # Add to existing scores, weighted\n",
    "                avg_stress_scores += normalized_values * reference_values[metric]['weight']\n",
    "    \n",
    "    # If we couldn't calculate stress from average data, return\n",
    "    if avg_stress is None:\n",
    "        print(\"Could not calculate average stress levels\")\n",
    "        return\n",
    "    \n",
    "    # Normalize by total weight\n",
    "    total_weight = sum(reference_values[m]['weight'] for m in stress_metrics)\n",
    "    if total_weight > 0:\n",
    "        avg_stress['stress_score'] = avg_stress_scores / total_weight\n",
    "    \n",
    "    # Now calculate individual participant stress\n",
    "    for participant_id, processed_data in processed_participants.items():\n",
    "        # Check if participant has required metrics\n",
    "        if not any(metric in processed_data for metric in stress_metrics):\n",
    "            print(f\"Participant {participant_id} is missing required stress metrics\")\n",
    "            continue\n",
    "            \n",
    "        participant_stress[participant_id] = None\n",
    "        stress_scores = None\n",
    "        \n",
    "        for metric in stress_metrics:\n",
    "            if metric in processed_data and metric in reference_values:\n",
    "                part_df = processed_data[metric]\n",
    "                max_normal = reference_values[metric]['max_normal']\n",
    "                min_stress = reference_values[metric]['min_stress']\n",
    "                \n",
    "                # Normalized values for this participant\n",
    "                normalized_values = np.clip(\n",
    "                    (part_df['value'] - max_normal) / (min_stress - max_normal),\n",
    "                    0, 1\n",
    "                )\n",
    "                \n",
    "                if participant_stress[participant_id] is None:\n",
    "                    participant_stress[participant_id] = part_df[['time_index', 'minutes_into_exam']].copy()\n",
    "                    stress_scores = normalized_values * reference_values[metric]['weight']\n",
    "                else:\n",
    "                    # Add if we can align the time indices\n",
    "                    stress_scores += normalized_values * reference_values[metric]['weight']\n",
    "        \n",
    "        # Normalize by total weight\n",
    "        if total_weight > 0 and stress_scores is not None:\n",
    "            participant_stress[participant_id]['stress_score'] = stress_scores / total_weight\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Define a color palette for participants\n",
    "    participant_colors = plt.cm.tab10(np.linspace(0, 1, len(sampled_participants)))\n",
    "    \n",
    "    # Plot average stress\n",
    "    plt.plot(\n",
    "        avg_stress['minutes_into_exam'],\n",
    "        avg_stress['stress_score'],\n",
    "        color='black',\n",
    "        linewidth=3,\n",
    "        label='Average Stress Level'\n",
    "    )\n",
    "    \n",
    "    # Plot individual participant stress\n",
    "    for i, (participant_id, stress_df) in enumerate(participant_stress.items()):\n",
    "        if stress_df is not None and 'stress_score' in stress_df.columns:\n",
    "            plt.plot(\n",
    "                stress_df['minutes_into_exam'],\n",
    "                stress_df['stress_score'],\n",
    "                color=participant_colors[i],\n",
    "                linewidth=1.5,\n",
    "                alpha=0.8,\n",
    "                label=f'Participant {participant_id}'\n",
    "            )\n",
    "    \n",
    "    # Add task markers\n",
    "    if exam_duration_minutes == 90:  # Midterm exam\n",
    "        markers = [\n",
    "            (0, \"Start\"),\n",
    "            (10, \"Task 1 (Lego without instructions)\"),\n",
    "            (17, \"Task 2 (Lego with instructions)\"),\n",
    "            (22, \"Task 3 (Lego + counting backwards)\"),\n",
    "            (35, \"Mathematical Task\"),\n",
    "            (45, \"Oral Presentation\"),\n",
    "            (90, \"End\")\n",
    "        ]\n",
    "    else:  # Final exam\n",
    "        markers = [\n",
    "            (0, \"Start\"),\n",
    "            (20, \"Task 1\"),\n",
    "            (34, \"Task 2\"),\n",
    "            (44, \"Task 3\"),\n",
    "            (70, \"Math Task\"),\n",
    "            (90, \"Oral Presentation\"),\n",
    "            (180, \"End\")\n",
    "        ]\n",
    "    \n",
    "    # Get current y limits\n",
    "    y_min, y_max = plt.ylim()\n",
    "    \n",
    "    for minute, label in markers:\n",
    "        if minute <= avg_stress['minutes_into_exam'].max():\n",
    "            plt.axvline(x=minute, linestyle=':', color='gray', alpha=0.7)\n",
    "            plt.text(minute, y_min - 0.05, label, rotation=90, verticalalignment='top', fontsize=8)\n",
    "    \n",
    "    # Add stress level indicators\n",
    "    plt.axhspan(0, 0.3, alpha=0.1, color='green', label='Low Stress')\n",
    "    plt.axhspan(0.3, 0.7, alpha=0.1, color='yellow', label='Moderate Stress')\n",
    "    plt.axhspan(0.7, 1, alpha=0.1, color='red', label='High Stress')\n",
    "    \n",
    "    # Set axis labels and title\n",
    "    plt.xlabel('Minutes into Exam')\n",
    "    plt.ylabel('Stress Level')\n",
    "    plt.title('Estimated Stress Levels During Exam')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add legend with smaller font and placed outside the plot\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=9)\n",
    "    \n",
    "    # Add grid\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_folder}/stress_level_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Saved stress level comparison plot to {output_folder}\")\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    raw_data_file = 'complete_dataset.json'\n",
    "    average_data_file = 'average_metrics.json'\n",
    "    output_folder = 'individual_plots'\n",
    "    \n",
    "    # Exam duration in minutes (from the paper: 90 minutes for midterms, 180 for final)\n",
    "    exam_duration_minutes = 90  # Change to 180 for final exam\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Load raw data\n",
    "    raw_data = load_data(raw_data_file)\n",
    "    if raw_data is None:\n",
    "        return\n",
    "    \n",
    "    # Load precomputed average data\n",
    "    average_data = load_average_data(average_data_file)\n",
    "    if average_data is None:\n",
    "        print(\"Average data not found. Please run the average computation script first.\")\n",
    "        return\n",
    "    \n",
    "    # Print columns in average data for debugging\n",
    "    for metric, df in average_data.items():\n",
    "        print(f\"Columns in {metric} average data: {df.columns.tolist()}\")\n",
    "        # If 'minutes_into_exam' is missing, add it\n",
    "        if 'minutes_into_exam' not in df.columns and 'time_index' in df.columns:\n",
    "            max_index = df['time_index'].max()\n",
    "            df['minutes_into_exam'] = df['time_index'] * (exam_duration_minutes / max_index)\n",
    "            print(f\"Added 'minutes_into_exam' column to {metric} data\")\n",
    "    \n",
    "    # Sample 5 random participants\n",
    "    sampled_participants = sample_participants(raw_data, num_samples=5)\n",
    "    print(f\"Sampled participants: {', '.join(sampled_participants)}\")\n",
    "    \n",
    "    # Create individual vs average plots\n",
    "    metrics_to_plot = ['EDA', 'HR', 'TEMP', 'BVP', 'IBI']  # Add or remove metrics as needed\n",
    "    plot_individual_vs_average(\n",
    "        raw_data, \n",
    "        average_data, \n",
    "        sampled_participants, \n",
    "        metrics_to_plot,\n",
    "        output_folder,\n",
    "        exam_duration_minutes\n",
    "    )\n",
    "    \n",
    "    # Create multi-participant comparison plot\n",
    "    create_multi_participant_comparison(\n",
    "        raw_data, \n",
    "        average_data, \n",
    "        sampled_participants, \n",
    "        metrics=['EDA', 'HR'],\n",
    "        output_folder=output_folder,\n",
    "        exam_duration_minutes=exam_duration_minutes\n",
    "    )\n",
    "    \n",
    "    # Create stress level comparison\n",
    "    create_stress_level_plot(\n",
    "        raw_data,\n",
    "        average_data,\n",
    "        sampled_participants,\n",
    "        output_folder=output_folder,\n",
    "        exam_duration_minutes=exam_duration_minutes\n",
    "    )\n",
    "    \n",
    "    print(\"Analysis complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating JSON file: complete_dataset.json\n",
      "File size: 225.39 MB\n",
      "Checking file format...\n",
      "Error parsing first 10KB: Expecting ',' delimiter: line 660 column 7 (char 10001)\n",
      "Error parsing complete file: Expecting property name enclosed in double quotes: line 13388360 column 9 (char 236337703)\n",
      "Error occurred at position 236337703, line 13388360, column 9\n",
      "\n",
      "Context around error:\n",
      "    Line 13388358:       },\n",
      "    Line 13388359:       {\n",
      ">>> Line 13388360: \n",
      "                 ^-- Error position\n",
      "\n",
      "Please provide an alternative path to your JSON data file:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 188\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Failed to save average metrics. Analysis aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 188\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[7], line 155\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# If loading failed, try to get alternative file path or use dummy data\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     alternative_path \u001b[38;5;241m=\u001b[39m get_alternative_json_path()\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m alternative_path:\n\u001b[1;32m    157\u001b[0m         data \u001b[38;5;241m=\u001b[39m load_data(alternative_path)\n",
      "Cell \u001b[0;32mIn[7], line 123\u001b[0m, in \u001b[0;36mget_alternative_json_path\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Ask user for alternative JSON file path.\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease provide an alternative path to your JSON data file:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 123\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter file path (or press Enter to use dummy data): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_path:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(file_path):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py:1175\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1174\u001b[0m     )\n\u001b[0;32m-> 1175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[1;32m   1176\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[1;32m   1177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1179\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1180\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py:1217\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1216\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "def get_metric_color(metric):\n",
    "    \"\"\"Return a consistent color for each metric.\"\"\"\n",
    "    colors = {\n",
    "        'EDA': '#1d3557',  # Dark blue\n",
    "        'HR': '#e63946',   # Red\n",
    "        'TEMP': '#457b9d', # Medium blue\n",
    "        'BVP': '#a8dadc',  # Light blue\n",
    "        'IBI': '#f1faee',  # Off-white\n",
    "        'ACC': '#e9c46a'   # Yellow\n",
    "    }\n",
    "    return colors.get(metric, '#666666')  # Gray for unknown metrics\n",
    "\n",
    "def get_stress_color(stress_level):\n",
    "    \"\"\"Return a color based on stress level.\"\"\"\n",
    "    colors = {\n",
    "        'Very Low': '#a8dadc',   # Light blue\n",
    "        'Low': '#90be6d',        # Green\n",
    "        'Moderate': '#f9c74f',   # Yellow\n",
    "        'High': '#f8961e',       # Orange\n",
    "        'Very High': '#e63946'   # Red\n",
    "    }\n",
    "    return colors.get(stress_level, '#666666')  # Gray for unknown levels\n",
    "\n",
    "def inspect_first_entry(data):\n",
    "    \"\"\"Inspect the first entry in the data to understand structure.\"\"\"\n",
    "    if not data:\n",
    "        print(\"No data to inspect\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        first_participant = list(data.keys())[0]\n",
    "        print(f\"\\nInspecting first participant ({first_participant}):\")\n",
    "        \n",
    "        participant_data = data[first_participant]\n",
    "        if not isinstance(participant_data, dict):\n",
    "            print(f\"  Warning: Participant data is not a dictionary, it's {type(participant_data)}\")\n",
    "            return\n",
    "        \n",
    "        for metric, values in participant_data.items():\n",
    "            print(f\"  Metric: {metric}\")\n",
    "            print(f\"  Type: {type(values)}\")\n",
    "            print(f\"  Length: {len(values) if hasattr(values, '__len__') else 'N/A'}\")\n",
    "            \n",
    "            if isinstance(values, list) and len(values) > 0:\n",
    "                print(f\"  First value type: {type(values[0])}\")\n",
    "                print(f\"  First value: {values[0]}\")\n",
    "                \n",
    "                if len(values) > 1:\n",
    "                    print(f\"  Second value type: {type(values[1])}\")\n",
    "                    print(f\"  Second value: {values[1]}\")\n",
    "        \n",
    "        print(\"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inspecting first entry: {str(e)}\")\n",
    "\n",
    "def validate_json_file(file_path):\n",
    "    \"\"\"Validate a JSON file and provide diagnostics.\"\"\"\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Error: File {file_path} does not exist\")\n",
    "            return False\n",
    "        \n",
    "        # Check file size\n",
    "        file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
    "        print(f\"File size: {file_size:.2f} MB\")\n",
    "        \n",
    "        # Try to read the first few lines\n",
    "        print(\"Checking file format...\")\n",
    "        with open(file_path, 'r') as f:\n",
    "            first_lines = [next(f) for _ in range(10) if f]\n",
    "        \n",
    "        # Check if it starts with a valid JSON marker\n",
    "        first_line = first_lines[0].strip() if first_lines else \"\"\n",
    "        \n",
    "        if not first_line.startswith('{') and not first_line.startswith('['):\n",
    "            print(\"Warning: File does not start with a valid JSON marker ({ or [)\")\n",
    "            \n",
    "        # Try to load a small portion of the file\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                content = f.read(10000)  # Read first 10KB\n",
    "                json.loads(content + ']' if content.startswith('[') else content + '}')\n",
    "                print(\"File appears to have valid JSON format at the beginning\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing first 10KB: {str(e)}\")\n",
    "        \n",
    "        # Try to validate full file\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                json.load(f)\n",
    "                print(\"Successfully validated the entire JSON file\")\n",
    "                return True\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing complete file: {str(e)}\")\n",
    "            print(f\"Error occurred at position {e.pos}, line {e.lineno}, column {e.colno}\")\n",
    "            \n",
    "            # Try to show problematic section\n",
    "            with open(file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                \n",
    "                # Show context around the error\n",
    "                start_line = max(0, e.lineno - 3)\n",
    "                end_line = min(len(lines), e.lineno + 3)\n",
    "                \n",
    "                print(\"\\nContext around error:\")\n",
    "                for i in range(start_line, end_line):\n",
    "                    prefix = \">>> \" if i + 1 == e.lineno else \"    \"\n",
    "                    print(f\"{prefix}Line {i+1}: {lines[i].rstrip()}\")\n",
    "                \n",
    "                # Show position indicator\n",
    "                if e.lineno <= len(lines):\n",
    "                    print(\" \" * (e.colno + 8) + \"^-- Error position\")\n",
    "                    \n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating JSON file: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def get_alternative_json_path():\n",
    "    \"\"\"Ask user for alternative JSON file path.\"\"\"\n",
    "    print(\"\\nPlease provide an alternative path to your JSON data file:\")\n",
    "    file_path = input(\"Enter file path (or press Enter to use dummy data): \").strip()\n",
    "    \n",
    "    if file_path:\n",
    "        if os.path.exists(file_path):\n",
    "            return file_path\n",
    "        else:\n",
    "            print(f\"File {file_path} does not exist\")\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # File paths\n",
    "    input_file = 'complete_dataset.json'\n",
    "    output_file = 'average_metrics_windowed.json'\n",
    "    output_folder = 'windowed_plots'\n",
    "    \n",
    "    # First, validate the JSON file\n",
    "    print(f\"Validating JSON file: {input_file}\")\n",
    "    is_valid = validate_json_file(input_file)\n",
    "    \n",
    "    # Window parameters\n",
    "    window_size = 30  # Number of data points per window\n",
    "    window_overlap = 0.5  # 50% overlap between windows\n",
    "    \n",
    "    # Load data with improved error handling\n",
    "    data = None\n",
    "    if is_valid:\n",
    "        data = load_data(input_file)\n",
    "    \n",
    "    # If loading failed, try to get alternative file path or use dummy data\n",
    "    if data is None:\n",
    "        alternative_path = get_alternative_json_path()\n",
    "        if alternative_path:\n",
    "            data = load_data(alternative_path)\n",
    "        \n",
    "        # If still no data, create dummy data for testing\n",
    "        if data is None:\n",
    "            data = create_dummy_data()\n",
    "    \n",
    "    # Inspect data structure to check first entry\n",
    "    inspect_first_entry(data)\n",
    "    \n",
    "    # Compute average metrics using windowing\n",
    "    averages = compute_windowed_average_metrics(data, window_size, window_overlap)\n",
    "    \n",
    "    if not averages:\n",
    "        print(\"Error: No average metrics computed. Cannot continue.\")\n",
    "        return\n",
    "    \n",
    "    # Save average metrics\n",
    "    success = save_average_metrics(averages, output_file)\n",
    "    \n",
    "    if success:\n",
    "        # Plot average metrics\n",
    "        plot_average_metrics(averages, output_folder)\n",
    "        \n",
    "        # Create multi-metric summary\n",
    "        create_multi_metric_summary(averages, output_folder)\n",
    "        \n",
    "        print(\"Analysis complete!\")\n",
    "    else:\n",
    "        print(\"Error: Failed to save average metrics. Analysis aborted.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing participants: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved data to complete_dataset.json\n",
      "File size: 0.00 MB\n",
      "Loaded data for 0 participants\n",
      "Available metrics: \n",
      "Saved average metrics to average_metrics.json\n",
      "File size: 0.00 KB\n",
      "Failed to calculate averages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_csv_to_json(data_dir='./Subjects/', max_participants=None):\n",
    "    \"\"\"\n",
    "    Convert participant CSV files to a JSON file using windowing approach from the paper.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_dir: Directory containing participant folders\n",
    "    - max_participants: Maximum number of participants to process (None for all)\n",
    "    \"\"\"\n",
    "    # Get list of participant folders\n",
    "    participant_folders = [f for f in os.listdir(data_dir) \n",
    "                          if os.path.isdir(os.path.join(data_dir, f)) \n",
    "                          and f.startswith('subject_')]\n",
    "    \n",
    "    # Sort folders to ensure consistent order\n",
    "    participant_folders.sort()\n",
    "    \n",
    "    # Limit number of participants if specified\n",
    "    if max_participants is not None:\n",
    "        participant_folders = participant_folders[:max_participants]\n",
    "    \n",
    "    # Key metrics mentioned in the paper\n",
    "    paper_metrics = ['EDA', 'HR', 'TEMP', 'BVP', 'IBI']\n",
    "    \n",
    "    # Dictionary to store all data\n",
    "    all_data = {}\n",
    "    \n",
    "    # Process each participant\n",
    "    for participant in tqdm(participant_folders, desc=\"Processing participants\"):\n",
    "        print(f\"Processing {participant}...\")\n",
    "        all_data[participant] = {}\n",
    "        \n",
    "        # Process each metric mentioned in the paper\n",
    "        for metric in paper_metrics:\n",
    "            csv_path = os.path.join(data_dir, participant, f\"{metric}.csv\")\n",
    "            \n",
    "            # Check if file exists\n",
    "            if not os.path.exists(csv_path):\n",
    "                print(f\"  ✗ {metric}.csv not found\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Load and process the CSV file\n",
    "                df = pd.read_csv(csv_path)\n",
    "                \n",
    "                # Simplify the data structure based on metric type\n",
    "                if metric == 'ACC':\n",
    "                    # For accelerometer data\n",
    "                    if len(df.columns) >= 3:\n",
    "                        # Extract x, y, z columns\n",
    "                        values = df.iloc[:, 0:3].values.tolist()\n",
    "                    else:\n",
    "                        # If columns are missing, use the available ones\n",
    "                        values = df.values.tolist()\n",
    "                elif metric == 'IBI':\n",
    "                    # For IBI data, get the IBI column if available\n",
    "                    if 'IBI' in df.columns:\n",
    "                        values = df['IBI'].values.tolist()\n",
    "                    else:\n",
    "                        # Otherwise just use the first column\n",
    "                        values = df.iloc[:, 0].values.tolist()\n",
    "                else:\n",
    "                    # For other metrics (EDA, HR, TEMP, BVP)\n",
    "                    values = df.iloc[:, 0].values.tolist()\n",
    "                \n",
    "                # Apply windowing as described in the paper\n",
    "                window_size = 30  # From paper: \"a 30-point window for EDA\"\n",
    "                window_overlap = 0.5  # 50% overlap between windows\n",
    "                \n",
    "                # Calculate step size based on overlap\n",
    "                step_size = int(window_size * (1 - window_overlap))\n",
    "                \n",
    "                # Ensure step size is at least 1\n",
    "                step_size = max(1, step_size)\n",
    "                \n",
    "                # Calculate number of windows\n",
    "                num_windows = (len(values) - window_size) // step_size + 1\n",
    "                \n",
    "                # Process each window\n",
    "                windowed_values = []\n",
    "                for window_idx in range(num_windows):\n",
    "                    start_idx = window_idx * step_size\n",
    "                    end_idx = start_idx + window_size\n",
    "                    \n",
    "                    # Ensure end index doesn't exceed data length\n",
    "                    end_idx = min(end_idx, len(values))\n",
    "                    \n",
    "                    # Get window data\n",
    "                    window_data = values[start_idx:end_idx]\n",
    "                    \n",
    "                    # For the JSON file, we're keeping the raw values, not averages\n",
    "                    # We'll compute averages in the analysis phase\n",
    "                    windowed_values.append(window_data)\n",
    "                \n",
    "                # Store the windowed data\n",
    "                all_data[participant][metric] = windowed_values\n",
    "                \n",
    "                print(f\"  ✓ Processed {metric}.csv - {len(windowed_values)} windows\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error processing {metric}.csv: {str(e)}\")\n",
    "    \n",
    "    # Save the data to a JSON file\n",
    "    output_filename = 'complete_dataset.json'\n",
    "    \n",
    "    try:\n",
    "        with open(output_filename, 'w') as f:\n",
    "            # Use compact JSON format (no pretty printing) to reduce file size\n",
    "            json.dump(all_data, f)\n",
    "        \n",
    "        print(f\"\\nSuccessfully saved data to {output_filename}\")\n",
    "        print(f\"File size: {os.path.getsize(output_filename) / (1024*1024):.2f} MB\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to {output_filename}: {str(e)}\")\n",
    "        \n",
    "        # Try saving to an alternative file with less indentation\n",
    "        try:\n",
    "            with open('complete_dataset_minimal.json', 'w') as f:\n",
    "                json.dump(all_data, f, separators=(',', ':'))\n",
    "            print(f\"Saved minimal version to complete_dataset_minimal.json\")\n",
    "            return True\n",
    "        except Exception as e2:\n",
    "            print(f\"Error saving minimal file: {str(e2)}\")\n",
    "            return False\n",
    "\n",
    "def process_windows_to_average(json_path='complete_dataset.json'):\n",
    "    \"\"\"\n",
    "    Process the windows in the dataset to calculate averages.\n",
    "    \n",
    "    This function reads the windowed data and calculates average metrics\n",
    "    according to the approach mentioned in the paper.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        print(f\"Loaded data for {len(data)} participants\")\n",
    "        \n",
    "        # Dictionary to store average metrics\n",
    "        averages = {}\n",
    "        \n",
    "        # Process each metric\n",
    "        metrics = set()\n",
    "        for participant_data in data.values():\n",
    "            for metric in participant_data.keys():\n",
    "                metrics.add(metric)\n",
    "        \n",
    "        print(f\"Available metrics: {', '.join(metrics)}\")\n",
    "        \n",
    "        for metric in metrics:\n",
    "            print(f\"Processing {metric}...\")\n",
    "            \n",
    "            # Collect all window values across participants\n",
    "            all_window_data = []\n",
    "            for participant, participant_data in data.items():\n",
    "                if metric in participant_data:\n",
    "                    for window_idx, window in enumerate(participant_data[metric]):\n",
    "                        # Each window is a list of values\n",
    "                        # We need to flatten the nested windows from multiple participants\n",
    "                        \n",
    "                        # For ACC data which might be multi-dimensional\n",
    "                        if metric == 'ACC' and isinstance(window[0], list):\n",
    "                            # Calculate magnitude for each point in the window\n",
    "                            magnitudes = []\n",
    "                            for point in window:\n",
    "                                if len(point) >= 3:\n",
    "                                    # Calculate magnitude of 3D vector\n",
    "                                    magnitude = np.sqrt(sum(v*v for v in point[:3]))\n",
    "                                    magnitudes.append(magnitude)\n",
    "                            \n",
    "                            # Store window index and magnitudes\n",
    "                            if magnitudes:\n",
    "                                all_window_data.append({\n",
    "                                    'participant': participant,\n",
    "                                    'window_idx': window_idx,\n",
    "                                    'values': magnitudes\n",
    "                                })\n",
    "                        else:\n",
    "                            # For other metrics, store the window values directly\n",
    "                            all_window_data.append({\n",
    "                                'participant': participant,\n",
    "                                'window_idx': window_idx,\n",
    "                                'values': window\n",
    "                            })\n",
    "            \n",
    "            print(f\"  Collected {len(all_window_data)} windows from all participants\")\n",
    "            \n",
    "            # Calculate average for each window position\n",
    "            window_averages = {}\n",
    "            for window_data in all_window_data:\n",
    "                window_idx = window_data['window_idx']\n",
    "                values = window_data['values']\n",
    "                \n",
    "                # Initialize window stats if not already present\n",
    "                if window_idx not in window_averages:\n",
    "                    window_averages[window_idx] = {\n",
    "                        'sum': sum(values),\n",
    "                        'count': len(values),\n",
    "                        'values': values\n",
    "                    }\n",
    "                else:\n",
    "                    # Update existing window stats\n",
    "                    window_averages[window_idx]['sum'] += sum(values)\n",
    "                    window_averages[window_idx]['count'] += len(values)\n",
    "                    window_averages[window_idx]['values'].extend(values)\n",
    "            \n",
    "            # Calculate final averages and standard deviations\n",
    "            result_data = []\n",
    "            for window_idx, stats in sorted(window_averages.items()):\n",
    "                if stats['count'] > 0:\n",
    "                    avg_value = stats['sum'] / stats['count']\n",
    "                    std_value = np.std(stats['values']) if len(stats['values']) > 1 else 0\n",
    "                    \n",
    "                    result_data.append({\n",
    "                        'window_index': window_idx,\n",
    "                        'average_value': avg_value,\n",
    "                        'std_value': std_value,\n",
    "                        'count': stats['count']\n",
    "                    })\n",
    "            \n",
    "            if result_data:\n",
    "                # Convert to DataFrame for easier manipulation\n",
    "                df = pd.DataFrame(result_data)\n",
    "                \n",
    "                # Add normalized time for visualization\n",
    "                max_window = df['window_index'].max()\n",
    "                if max_window > 0:\n",
    "                    df['normalized_time'] = df['window_index'] / max_window\n",
    "                else:\n",
    "                    df['normalized_time'] = 0.5\n",
    "                \n",
    "                # Add minutes_into_exam (assuming 90-minute exam as in the paper)\n",
    "                df['minutes_into_exam'] = df['normalized_time'] * 90\n",
    "                \n",
    "                # Store the processed data\n",
    "                averages[metric] = df\n",
    "                \n",
    "                print(f\"  Processed {len(df)} windows with averages for {metric}\")\n",
    "            else:\n",
    "                print(f\"  No valid windows processed for {metric}\")\n",
    "        \n",
    "        # Save average metrics to a JSON file\n",
    "        output_file = 'average_metrics.json'\n",
    "        output_data = {}\n",
    "        \n",
    "        for metric, df in averages.items():\n",
    "            output_data[metric] = df.to_dict(orient='records')\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(output_data, f, indent=2)\n",
    "        \n",
    "        print(f\"Saved average metrics to {output_file}\")\n",
    "        print(f\"File size: {os.path.getsize(output_file) / 1024:.2f} KB\")\n",
    "        \n",
    "        return averages\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing windows to average: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Set the directory containing participant folders\n",
    "    data_dir = '.'  # Current directory - update if your data is in a subfolder\n",
    "    \n",
    "    # Optional: Limit to a specific number of participants for testing\n",
    "    max_participants = None  # Set to a number (e.g., 10) to limit, or None for all\n",
    "    \n",
    "    # Convert CSV files to JSON\n",
    "    success = convert_csv_to_json(data_dir, max_participants)\n",
    "    \n",
    "    if success:\n",
    "        # Process the JSON data to calculate averages\n",
    "        averages = process_windows_to_average()\n",
    "        \n",
    "        if averages:\n",
    "            print(\"Dataset and averages successfully generated!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Failed to calculate averages.\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"Failed to generate dataset.\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29 participant folders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing participants:  14%|█▍        | 4/29 [00:00<00:00, 33.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subject_01...\n",
      "  ✓ Processed EDA.csv - 10019 data points\n",
      "  ✓ Processed HR.csv - 2495 data points\n",
      "  ✓ Processed TEMP.csv - 10015 data points\n",
      "  ✓ Processed BVP.csv - 160368 data points\n",
      "  ✓ Processed IBI.csv - 449 data points\n",
      "Processing subject_02...\n",
      "  ✓ Processed EDA.csv - 9419 data points\n",
      "  ✓ Processed HR.csv - 2345 data points\n",
      "  ✓ Processed TEMP.csv - 9425 data points\n",
      "  ✓ Processed BVP.csv - 150776 data points\n",
      "  ✓ Processed IBI.csv - 209 data points\n",
      "Processing subject_03...\n",
      "  ✓ Processed EDA.csv - 8303 data points\n",
      "  ✓ Processed HR.csv - 2066 data points\n",
      "  ✓ Processed TEMP.csv - 8305 data points\n",
      "  ✓ Processed BVP.csv - 132879 data points\n",
      "  ✓ Processed IBI.csv - 690 data points\n",
      "Processing subject_04...\n",
      "  ✓ Processed EDA.csv - 8435 data points\n",
      "  ✓ Processed HR.csv - 2098 data points\n",
      "  ✓ Processed TEMP.csv - 8433 data points\n",
      "  ✓ Processed BVP.csv - 134958 data points\n",
      "  ✓ Processed IBI.csv - 570 data points\n",
      "Processing subject_05...\n",
      "  ✓ Processed EDA.csv - 8333 data points\n",
      "  ✓ Processed HR.csv - 2073 data points\n",
      "  ✓ Processed TEMP.csv - 8337 data points\n",
      "  ✓ Processed BVP.csv - 133363 data points\n",
      "  ✓ Processed IBI.csv - 750 data points\n",
      "Processing subject_06...\n",
      "  ✓ Processed EDA.csv - 8651 data points\n",
      "  ✓ Processed HR.csv - 2153 data points\n",
      "  ✓ Processed TEMP.csv - 8657 data points\n",
      "  ✓ Processed BVP.csv - 138478 data points\n",
      "  ✓ Processed IBI.csv - 330 data points\n",
      "Processing subject_07...\n",
      "  ✓ Processed EDA.csv - 8573 data points\n",
      "  ✓ Processed HR.csv - 2133 data points\n",
      "  ✓ Processed TEMP.csv - 8577 data points\n",
      "  ✓ Processed BVP.csv - 137191 data points\n",
      "  ✓ Processed IBI.csv - 30 data points\n",
      "Processing subject_08...\n",
      "  ✓ Processed EDA.csv - 8885 data points\n",
      "  ✓ Processed HR.csv - 2211 data points\n",
      "  ✓ Processed TEMP.csv - 8881 data points\n",
      "  ✓ Processed BVP.csv - 142152 data points\n",
      "  ✓ Processed IBI.csv - 240 data points\n",
      "Processing subject_09...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing participants:  55%|█████▌    | 16/29 [00:00<00:00, 50.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Processed EDA.csv - 8885 data points\n",
      "  ✓ Processed HR.csv - 2211 data points\n",
      "  ✓ Processed TEMP.csv - 8881 data points\n",
      "  ✓ Processed BVP.csv - 142152 data points\n",
      "  ✓ Processed IBI.csv - 1290 data points\n",
      "Processing subject_10...\n",
      "  ✓ Processed EDA.csv - 8831 data points\n",
      "  ✓ Processed HR.csv - 2198 data points\n",
      "  ✓ Processed TEMP.csv - 8833 data points\n",
      "  ✓ Processed BVP.csv - 141349 data points\n",
      "  ✓ Processed IBI.csv - 90 data points\n",
      "Processing subject_11...\n",
      "  ✓ Processed EDA.csv - 8483 data points\n",
      "  ✓ Processed HR.csv - 2111 data points\n",
      "  ✓ Processed TEMP.csv - 8481 data points\n",
      "  ✓ Processed BVP.csv - 135761 data points\n",
      "  ✓ Processed IBI.csv - 1350 data points\n",
      "Processing subject_12...\n",
      "  ✓ Processed EDA.csv - 8921 data points\n",
      "  ✓ Processed HR.csv - 2220 data points\n",
      "  ✓ Processed TEMP.csv - 8921 data points\n",
      "  ✓ Processed BVP.csv - 142790 data points\n",
      "  ✓ Processed IBI.csv - 210 data points\n",
      "Processing subject_13...\n",
      "  ✓ Processed EDA.csv - 8771 data points\n",
      "  ✓ Processed HR.csv - 2183 data points\n",
      "  ✓ Processed TEMP.csv - 8777 data points\n",
      "  ✓ Processed BVP.csv - 140392 data points\n",
      "  ✓ Processed IBI.csv - 360 data points\n",
      "Processing subject_14...\n",
      "  ✓ Processed EDA.csv - 10367 data points\n",
      "  ✓ Processed HR.csv - 2582 data points\n",
      "  ✓ Processed TEMP.csv - 10369 data points\n",
      "  ✓ Processed BVP.csv - 165956 data points\n",
      "  ✓ Processed IBI.csv - 570 data points\n",
      "Processing subject_15...\n",
      "  ✓ Processed EDA.csv - 9119 data points\n",
      "  ✓ Processed HR.csv - 2270 data points\n",
      "  ✓ Processed TEMP.csv - 9121 data points\n",
      "  ✓ Processed BVP.csv - 145980 data points\n",
      "  ✓ Processed IBI.csv - 780 data points\n",
      "Processing subject_16...\n",
      "  ✓ Processed EDA.csv - 8405 data points\n",
      "  ✓ Processed HR.csv - 2091 data points\n",
      "  ✓ Processed TEMP.csv - 8401 data points\n",
      "  ✓ Processed BVP.csv - 134485 data points\n",
      "  ✓ Processed IBI.csv - 150 data points\n",
      "Processing subject_17...\n",
      "  ✓ Processed EDA.csv - 9593 data points\n",
      "  ✓ Processed HR.csv - 2388 data points\n",
      "  ✓ Processed TEMP.csv - 9593 data points\n",
      "  ✓ Processed BVP.csv - 153482 data points\n",
      "  ✓ Processed IBI.csv - 150 data points\n",
      "Processing subject_18...\n",
      "  ✓ Processed EDA.csv - 8411 data points\n",
      "  ✓ Processed HR.csv - 2095 data points\n",
      "  ✓ Processed TEMP.csv - 8417 data points\n",
      "  ✓ Processed BVP.csv - 134639 data points\n",
      "  ✓ Processed IBI.csv - 360 data points\n",
      "Processing subject_19...\n",
      "  ✓ Processed EDA.csv - 8465 data points\n",
      "  ✓ Processed HR.csv - 2108 data points\n",
      "  ✓ Processed TEMP.csv - 8465 data points\n",
      "  ✓ Processed BVP.csv - 135442 data points\n",
      "  ✓ Processed IBI.csv - 300 data points\n",
      "Processing subject_20...\n",
      "  ✓ Processed EDA.csv - 8585 data points\n",
      "  ✓ Processed HR.csv - 2138 data points\n",
      "  ✓ Processed TEMP.csv - 8585 data points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing participants: 100%|██████████| 29/29 [00:00<00:00, 49.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Processed BVP.csv - 137356 data points\n",
      "  ✓ Processed IBI.csv - 870 data points\n",
      "Processing subject_21...\n",
      "  ✓ Processed EDA.csv - 8681 data points\n",
      "  ✓ Processed HR.csv - 2163 data points\n",
      "  ✓ Processed TEMP.csv - 8681 data points\n",
      "  ✓ Processed BVP.csv - 138962 data points\n",
      "  ✓ Processed IBI.csv - 480 data points\n",
      "Processing subject_22...\n",
      "  ✓ Processed EDA.csv - 8375 data points\n",
      "  ✓ Processed HR.csv - 2085 data points\n",
      "  ✓ Processed TEMP.csv - 8377 data points\n",
      "  ✓ Processed BVP.csv - 134001 data points\n",
      "  ✓ Processed IBI.csv - 1020 data points\n",
      "Processing subject_23...\n",
      "  ✓ Processed EDA.csv - 8213 data points\n",
      "  ✓ Processed HR.csv - 2045 data points\n",
      "  ✓ Processed TEMP.csv - 8217 data points\n",
      "  ✓ Processed BVP.csv - 131438 data points\n",
      "  ✓ Processed IBI.csv - 630 data points\n",
      "Processing subject_24...\n",
      "  ✓ Processed EDA.csv - 8333 data points\n",
      "  ✓ Processed HR.csv - 2075 data points\n",
      "  ✓ Processed TEMP.csv - 8337 data points\n",
      "  ✓ Processed BVP.csv - 133363 data points\n",
      "  ✓ Processed IBI.csv - 870 data points\n",
      "Processing subject_25...\n",
      "  ✓ Processed EDA.csv - 10001 data points\n",
      "  ✓ Processed HR.csv - 2492 data points\n",
      "  ✓ Processed TEMP.csv - 10001 data points\n",
      "  ✓ Processed BVP.csv - 160038 data points\n",
      "  ✓ Processed IBI.csv - 1380 data points\n",
      "Processing subject_26...\n",
      "  ✓ Processed EDA.csv - 10619 data points\n",
      "  ✓ Processed HR.csv - 2647 data points\n",
      "  ✓ Processed TEMP.csv - 10617 data points\n",
      "  ✓ Processed BVP.csv - 169949 data points\n",
      "  ✓ Processed IBI.csv - 1320 data points\n",
      "Processing subject_27...\n",
      "  ✓ Processed EDA.csv - 8405 data points\n",
      "  ✓ Processed HR.csv - 2093 data points\n",
      "  ✓ Processed TEMP.csv - 8401 data points\n",
      "  ✓ Processed BVP.csv - 134485 data points\n",
      "  ✓ Processed IBI.csv - 270 data points\n",
      "Processing subject_28...\n",
      "  ✓ Processed EDA.csv - 10001 data points\n",
      "  ✓ Processed HR.csv - 2492 data points\n",
      "  ✓ Processed TEMP.csv - 10001 data points\n",
      "  ✓ Processed BVP.csv - 160038 data points\n",
      "  ✓ Processed IBI.csv - 1770 data points\n",
      "Processing subject_29...\n",
      "  ✓ Processed EDA.csv - 9323 data points\n",
      "  ✓ Processed HR.csv - 2322 data points\n",
      "  ✓ Processed TEMP.csv - 9321 data points\n",
      "  ✓ Processed BVP.csv - 149181 data points\n",
      "  ✓ Processed IBI.csv - 450 data points\n",
      "\n",
      "Saving formatted JSON to complete_dataset.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved formatted data to complete_dataset.json\n",
      "File size: 61.43 MB\n",
      "Loaded data for 29 participants\n",
      "Available metrics: TEMP, IBI, EDA, HR, BVP\n",
      "Processing TEMP with window size 30, step size 15...\n",
      "  Maximum data length: 10617, will create 706 windows\n",
      "  Processed 706 windows with averages for TEMP\n",
      "Processing IBI with window size 30, step size 15...\n",
      "  Maximum data length: 1770, will create 117 windows\n",
      "  Processed 117 windows with averages for IBI\n",
      "Processing EDA with window size 30, step size 15...\n",
      "  Maximum data length: 10619, will create 706 windows\n",
      "  Processed 668 windows with averages for EDA\n",
      "Processing HR with window size 30, step size 15...\n",
      "  Maximum data length: 2647, will create 175 windows\n",
      "  Processed 175 windows with averages for HR\n",
      "Processing BVP with window size 30, step size 15...\n",
      "  Maximum data length: 169949, will create 11328 windows\n",
      "  Processed 11328 windows with averages for BVP\n",
      "Saved average metrics to average_metrics.json\n",
      "File size: 2947.42 KB\n",
      "Dataset and averages successfully generated with proper formatting!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_csv_to_json(data_dir='subjects', max_participants=None, indent=2):\n",
    "    \"\"\"\n",
    "    Convert participant CSV files to a well-formatted JSON file.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_dir: Directory containing participant folders (default: 'subjects')\n",
    "    - max_participants: Maximum number of participants to process (None for all)\n",
    "    - indent: Number of spaces for JSON indentation (default: 2)\n",
    "    \"\"\"\n",
    "    # Check if the subjects directory exists\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Error: Directory '{data_dir}' not found. Please check the path.\")\n",
    "        return False\n",
    "    \n",
    "    # Get list of participant folders within the subjects directory\n",
    "    participant_folders = [f for f in os.listdir(data_dir) \n",
    "                          if os.path.isdir(os.path.join(data_dir, f))]\n",
    "    \n",
    "    # Sort folders to ensure consistent order\n",
    "    participant_folders.sort()\n",
    "    \n",
    "    # Limit number of participants if specified\n",
    "    if max_participants is not None:\n",
    "        participant_folders = participant_folders[:max_participants]\n",
    "    \n",
    "    print(f\"Found {len(participant_folders)} participant folders\")\n",
    "    \n",
    "    # Key metrics mentioned in the paper\n",
    "    paper_metrics = ['EDA', 'HR', 'TEMP', 'BVP', 'IBI']\n",
    "    \n",
    "    # Dictionary to store all data\n",
    "    all_data = {}\n",
    "    \n",
    "    # Process each participant\n",
    "    for participant in tqdm(participant_folders, desc=\"Processing participants\"):\n",
    "        print(f\"Processing {participant}...\")\n",
    "        all_data[participant] = {}\n",
    "        \n",
    "        # Process each metric mentioned in the paper\n",
    "        for metric in paper_metrics:\n",
    "            csv_path = os.path.join(data_dir, participant, f\"{metric}.csv\")\n",
    "            \n",
    "            # Check if file exists\n",
    "            if not os.path.exists(csv_path):\n",
    "                print(f\"  ✗ {metric}.csv not found\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Load and process the CSV file\n",
    "                df = pd.read_csv(csv_path)\n",
    "                \n",
    "                # Simplify the data structure based on metric type\n",
    "                if metric == 'ACC':\n",
    "                    # For accelerometer data\n",
    "                    if len(df.columns) >= 3:\n",
    "                        # Extract x, y, z columns\n",
    "                        values = df.iloc[:, 0:3].values.tolist()\n",
    "                    else:\n",
    "                        # If columns are missing, use the available ones\n",
    "                        values = df.values.tolist()\n",
    "                elif metric == 'IBI':\n",
    "                    # For IBI data, get the IBI column if available\n",
    "                    if 'IBI' in df.columns:\n",
    "                        values = df['IBI'].values.tolist()\n",
    "                    else:\n",
    "                        # Otherwise just use the first column\n",
    "                        values = df.iloc[:, 0].values.tolist()\n",
    "                else:\n",
    "                    # For other metrics (EDA, HR, TEMP, BVP)\n",
    "                    values = df.iloc[:, 0].values.tolist()\n",
    "                \n",
    "                # Store the raw data (no windowing in the raw data file)\n",
    "                all_data[participant][metric] = values\n",
    "                \n",
    "                print(f\"  ✓ Processed {metric}.csv - {len(values)} data points\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error processing {metric}.csv: {str(e)}\")\n",
    "    \n",
    "    # Save the data to a JSON file with proper formatting\n",
    "    output_filename = 'complete_dataset.json'\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nSaving formatted JSON to {output_filename}...\")\n",
    "        \n",
    "        # Write the JSON file with proper indentation\n",
    "        with open(output_filename, 'w') as f:\n",
    "            json.dump(all_data, f, indent=indent)\n",
    "        \n",
    "        print(f\"Successfully saved formatted data to {output_filename}\")\n",
    "        print(f\"File size: {os.path.getsize(output_filename) / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        # If the file is extremely large, offer to create a minified version as well\n",
    "        file_size_mb = os.path.getsize(output_filename) / (1024*1024)\n",
    "        if file_size_mb > 100:  # If larger than 100MB\n",
    "            print(\"\\nWarning: The formatted JSON file is quite large.\")\n",
    "            create_minified = input(\"Would you like to create a minified version as well? (y/n): \").lower() == 'y'\n",
    "            \n",
    "            if create_minified:\n",
    "                minified_filename = 'complete_dataset_min.json'\n",
    "                with open(minified_filename, 'w') as f:\n",
    "                    json.dump(all_data, f, separators=(',', ':'))\n",
    "                print(f\"Saved minified version to {minified_filename}\")\n",
    "                print(f\"Minified file size: {os.path.getsize(minified_filename) / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to {output_filename}: {str(e)}\")\n",
    "        \n",
    "        # Try saving to an alternative file with less formatting\n",
    "        try:\n",
    "            print(\"\\nAttempting to save with minimal formatting...\")\n",
    "            with open('complete_dataset_min.json', 'w') as f:\n",
    "                json.dump(all_data, f, separators=(',', ':'))\n",
    "            print(f\"Saved minimal version to complete_dataset_min.json\")\n",
    "            return True\n",
    "        except Exception as e2:\n",
    "            print(f\"Error saving minimal file: {str(e2)}\")\n",
    "            return False\n",
    "\n",
    "def split_large_json(filename='complete_dataset.json', max_participants_per_file=5):\n",
    "    \"\"\"\n",
    "    Split a large JSON file into smaller parts.\n",
    "    Useful if the complete dataset is too large to work with.\n",
    "    \n",
    "    Parameters:\n",
    "    - filename: Path to the large JSON file\n",
    "    - max_participants_per_file: Maximum number of participants per split file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        print(f\"Loaded data for {len(data)} participants\")\n",
    "        \n",
    "        # Get participant IDs\n",
    "        participants = list(data.keys())\n",
    "        \n",
    "        # Calculate number of files needed\n",
    "        num_files = (len(participants) + max_participants_per_file - 1) // max_participants_per_file\n",
    "        \n",
    "        print(f\"Splitting into {num_files} files with {max_participants_per_file} participants each...\")\n",
    "        \n",
    "        # Create splits\n",
    "        for i in range(num_files):\n",
    "            start_idx = i * max_participants_per_file\n",
    "            end_idx = min((i + 1) * max_participants_per_file, len(participants))\n",
    "            \n",
    "            split_participants = participants[start_idx:end_idx]\n",
    "            split_data = {p: data[p] for p in split_participants}\n",
    "            \n",
    "            # Save split\n",
    "            split_filename = f'dataset_part_{i+1}of{num_files}.json'\n",
    "            with open(split_filename, 'w') as f:\n",
    "                json.dump(split_data, f, indent=2)\n",
    "            \n",
    "            print(f\"Saved {len(split_participants)} participants to {split_filename}\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error splitting file: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def process_windowed_averages(json_path='complete_dataset.json', window_size=30, window_overlap=0.5, indent=2):\n",
    "    \"\"\"\n",
    "    Process the dataset to calculate windowed averages.\n",
    "    \n",
    "    This function reads the raw data and applies the windowing technique\n",
    "    from the paper to calculate average metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    - json_path: Path to the JSON file with raw data\n",
    "    - window_size: Size of the window in data points (default: 30 from paper)\n",
    "    - window_overlap: Overlap between consecutive windows (default: 0.5 or 50%)\n",
    "    - indent: Number of spaces for JSON indentation (default: 2)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        print(f\"Loaded data for {len(data)} participants\")\n",
    "        \n",
    "        # Dictionary to store average metrics\n",
    "        averages = {}\n",
    "        \n",
    "        # Get all available metrics\n",
    "        metrics = set()\n",
    "        for participant_data in data.values():\n",
    "            for metric in participant_data.keys():\n",
    "                metrics.add(metric)\n",
    "        \n",
    "        print(f\"Available metrics: {', '.join(metrics)}\")\n",
    "        \n",
    "        # Calculate step size based on overlap\n",
    "        step_size = int(window_size * (1 - window_overlap))\n",
    "        if step_size < 1:\n",
    "            step_size = 1\n",
    "        \n",
    "        # Process each metric\n",
    "        for metric in metrics:\n",
    "            print(f\"Processing {metric} with window size {window_size}, step size {step_size}...\")\n",
    "            \n",
    "            # Determine the maximum length for this metric across all participants\n",
    "            max_length = 0\n",
    "            for participant_data in data.values():\n",
    "                if metric in participant_data:\n",
    "                    max_length = max(max_length, len(participant_data[metric]))\n",
    "            \n",
    "            if max_length == 0:\n",
    "                print(f\"  No data found for {metric}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate number of windows\n",
    "            num_windows = (max_length - window_size) // step_size + 1\n",
    "            if num_windows < 1:\n",
    "                num_windows = 1\n",
    "            \n",
    "            print(f\"  Maximum data length: {max_length}, will create {num_windows} windows\")\n",
    "            \n",
    "            # Initialize arrays to store window data\n",
    "            window_data = {\n",
    "                'window_index': [],\n",
    "                'average_value': [],\n",
    "                'std_value': [],\n",
    "                'count': []\n",
    "            }\n",
    "            \n",
    "            # Process each window\n",
    "            for window_idx in range(num_windows):\n",
    "                start_idx = window_idx * step_size\n",
    "                end_idx = start_idx + window_size\n",
    "                \n",
    "                # Adjust end index if it exceeds max length\n",
    "                if end_idx > max_length:\n",
    "                    end_idx = max_length\n",
    "                \n",
    "                # Collect values from all participants for this window\n",
    "                window_values = []\n",
    "                \n",
    "                for participant, participant_data in data.items():\n",
    "                    if metric in participant_data:\n",
    "                        participant_values = participant_data[metric]\n",
    "                        \n",
    "                        # Only consider windows that fit within this participant's data\n",
    "                        if start_idx < len(participant_values):\n",
    "                            # Adjust end index for this participant\n",
    "                            participant_end_idx = min(end_idx, len(participant_values))\n",
    "                            \n",
    "                            # Extract values for this window\n",
    "                            for i in range(start_idx, participant_end_idx):\n",
    "                                try:\n",
    "                                    value = participant_values[i]\n",
    "                                    \n",
    "                                    # Extract numeric value based on data type\n",
    "                                    numeric_value = None\n",
    "                                    if isinstance(value, (int, float)):\n",
    "                                        numeric_value = float(value)\n",
    "                                    elif isinstance(value, list) and len(value) > 0:\n",
    "                                        # For accelerometer data (3D vectors)\n",
    "                                        if len(value) >= 3:\n",
    "                                            numeric_value = np.sqrt(sum(float(v)**2 for v in value[:3]))\n",
    "                                        else:\n",
    "                                            numeric_value = float(value[0])\n",
    "                                    \n",
    "                                    if numeric_value is not None:\n",
    "                                        window_values.append(numeric_value)\n",
    "                                except (IndexError, TypeError, ValueError) as e:\n",
    "                                    # Skip problematic values\n",
    "                                    continue\n",
    "                \n",
    "                # Calculate statistics if we have values\n",
    "                if window_values:\n",
    "                    window_data['window_index'].append(window_idx)\n",
    "                    window_data['average_value'].append(np.mean(window_values))\n",
    "                    window_data['std_value'].append(np.std(window_values))\n",
    "                    window_data['count'].append(len(window_values))\n",
    "            \n",
    "            # Create DataFrame\n",
    "            if window_data['window_index']:\n",
    "                df = pd.DataFrame(window_data)\n",
    "                # Add normalized time for visualization\n",
    "                if len(df) > 1:\n",
    "                    df['normalized_time'] = df['window_index'] / df['window_index'].max()\n",
    "                else:\n",
    "                    df['normalized_time'] = 0.5  # Middle point if only one window\n",
    "                \n",
    "                # Add minutes_into_exam field (estimated based on 90-minute exam from paper)\n",
    "                df['minutes_into_exam'] = df['normalized_time'] * 90\n",
    "                \n",
    "                # Store the processed data\n",
    "                averages[metric] = df\n",
    "                \n",
    "                print(f\"  Processed {len(df)} windows with averages for {metric}\")\n",
    "            else:\n",
    "                print(f\"  No valid windows processed for {metric}\")\n",
    "        \n",
    "        # Save average metrics to a JSON file with nice formatting\n",
    "        output_file = 'average_metrics.json'\n",
    "        output_data = {}\n",
    "        \n",
    "        for metric, df in averages.items():\n",
    "            output_data[metric] = df.to_dict(orient='records')\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(output_data, f, indent=indent)\n",
    "        \n",
    "        print(f\"Saved average metrics to {output_file}\")\n",
    "        print(f\"File size: {os.path.getsize(output_file) / 1024:.2f} KB\")\n",
    "        \n",
    "        return averages\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing windowed averages: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Set the directory containing participant folders\n",
    "    data_dir = 'subjects'  # Subfolder containing participant data\n",
    "    \n",
    "    # Optional: Limit to a specific number of participants for testing\n",
    "    max_participants = None  # Set to a number (e.g., 10) to limit, or None for all\n",
    "    \n",
    "    # JSON indentation level (set to 2 or 4 spaces for nice formatting)\n",
    "    indent_level = 2\n",
    "    \n",
    "    # Convert CSV files to JSON with proper formatting\n",
    "    success = convert_csv_to_json(data_dir, max_participants, indent=indent_level)\n",
    "    \n",
    "    if success:\n",
    "        # Check if the file is too large and needs to be split\n",
    "        file_size_mb = os.path.getsize('complete_dataset.json') / (1024*1024)\n",
    "        if file_size_mb > 200:  # If larger than 200MB\n",
    "            print(f\"\\nWarning: The dataset file is quite large ({file_size_mb:.2f} MB).\")\n",
    "            split_file = input(\"Would you like to split it into smaller parts? (y/n): \").lower() == 'y'\n",
    "            \n",
    "            if split_file:\n",
    "                split_large_json(max_participants_per_file=5)\n",
    "        \n",
    "        # Process the JSON data to calculate windowed averages\n",
    "        # Using window parameters from the paper: 30-point window with 50% overlap\n",
    "        averages = process_windowed_averages(\n",
    "            window_size=30, \n",
    "            window_overlap=0.5,\n",
    "            indent=indent_level\n",
    "        )\n",
    "        \n",
    "        if averages:\n",
    "            print(\"Dataset and averages successfully generated with proper formatting!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Failed to calculate averages.\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"Failed to generate dataset.\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
